{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreventAttacks_FederatedLearning_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajattri/CSC591/blob/master/PreventAttacks_FederatedLearning_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHK8rjFlfjl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############\n",
        "#INSTALL PySyft\n",
        "##########\n",
        "\n",
        "!git clone https://github.com/OpenMined/PySyft.git\n",
        "!cd PySyft/\n",
        "!pip install -r PySyft/pip-dep/requirements.txt\n",
        "!pip install -r PySyft/pip-dep/requirements_udacity.txt\n",
        "!python PySyft/setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59BrMbq6ug4K",
        "colab_type": "text"
      },
      "source": [
        "Before Running the next step make sure to \"Restart Runtime\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX-oDlqVftfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell to add PySyft path \n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('./PySyft'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kJ6-PAifwhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKD0tBEGuvWZ",
        "colab_type": "text"
      },
      "source": [
        "Import PySyft and create hook to use torch libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adi3kkgifygA",
        "colab_type": "code",
        "outputId": "5469a8ce-8a8e-450a-9c53-7240f791f556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import syft as sy  # <-- NEW: import the Pysyft library\n",
        "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Torch was already hooked... skipping hooking process\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8BoTzYRu4MA",
        "colab_type": "text"
      },
      "source": [
        "Create Four agents. Each of them will be used to train on the data independently (sequentially though!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nonSaE52gTpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Agent_1 = sy.VirtualWorker(hook, id=\"Agent_1\")\n",
        "Agent_2 = sy.VirtualWorker(hook, id=\"Agent_2\")\n",
        "Agent_3 = sy.VirtualWorker(hook, id=\"Agent_3\")\n",
        "Agent_4 = sy.VirtualWorker(hook, id=\"Agent_4\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KQw4SNvvELE",
        "colab_type": "text"
      },
      "source": [
        "Arguments to be used for training. I have taken this directly from the PySyft tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOHndFeLf04a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 1\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H50kLyJDf-ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download dataset\n",
        "\n",
        "mnist_full_train_dataset = datasets.MNIST('../data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQyOOMxw0WhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find indexes of data points where class lables is 5 or 7 in the last dataset, i.e. index >52000\n",
        "# For the Fashion-MNIST dataset, the class ‘5’ (sandal) will be misclassified as class ‘7’ (sneaker) in ds_t3 by agent 3. \n",
        "# Agent 3 will take data from 56000 - 58000 range in time stamp 3\n",
        "\n",
        "indexes_5 = []\n",
        "\n",
        "for i in range(56000,58001):\n",
        "  if mnist_full_train_dataset.targets[i] == 5:\n",
        "    indexes_5.append(i)\n",
        "  else:\n",
        "    pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6N4VxTA2K83",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c4f5ec08-55d6-4e7a-91f3-aeaead55ad85"
      },
      "source": [
        "indexes_5[2]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56014"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucrXD2lc0kdM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e4e35687-88bc-4151-f151-5ddcba7193eb"
      },
      "source": [
        "# Test if the class lables were changed for some random data point 10\n",
        "\n",
        "print('Before change',mnist_full_train_dataset.targets[indexes_5[10]]) # this should be tensor(5)\n",
        "\n",
        "# lets change class lables in the subset from 5 to 7\n",
        "for i in range(0,len(indexes_5)):\n",
        "  mnist_full_train_dataset.targets[indexes_5[i]] = 7\n",
        "\n",
        "print('After change',mnist_full_train_dataset.targets[indexes_5[10]]) # this should be tensor(7)\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before change tensor(5)\n",
            "After change tensor(7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQLIHCsgvQ2s",
        "colab_type": "text"
      },
      "source": [
        "Split the dataset into three parts. This is done to mimic training on the data at three different \"time stamps\". The first dataset will be used to train the model with no malicious data. We will include some malicious data in datasets 2 and 3 and make sure the model obtained from dataset 1 is not updated by a malicious agent's updated delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3URcXVdggKlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "ds_t1,ds_t2,ds_t3 = torch.utils.data.random_split(mnist_full_train_dataset,(40000,12000,8000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyMVXrL2vy4A",
        "colab_type": "text"
      },
      "source": [
        "Create three \"federated\" datastes. Each of these federated datasets will be used for training at three different timetamps. Each dataset is nothing but a collection of a bacth of 64 training data items and the corresponding agent name where the batch will be sent for training. All initial batches belong to Agent1, the subsequent ones belong to Agent2, and so on.\n",
        "\n",
        "The reulting federated datasets are in this format:\n",
        "[(DATA[0:64], Agent_1),(DATA[64:128],Agent_1).....(DATA[10000:10064],Agent_2).....]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR-DEUvogLOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "federated_train_ds_t1 = sy.FederatedDataLoader( ds_t1.federate((Agent_1,Agent_2,Agent_3,Agent_4)),batch_size=64,shuffle=True, **kwargs)\n",
        "federated_train_ds_t2 = sy.FederatedDataLoader( ds_t2.federate((Agent_1,Agent_2,Agent_3,Agent_4)),batch_size=64,shuffle=True, **kwargs)\n",
        "federated_train_ds_t3 = sy.FederatedDataLoader( ds_t3.federate((Agent_1,Agent_2,Agent_3,Agent_4)),batch_size=64,shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4cwnrLhgRld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the dataset that will be used repeateadly to test model performance\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU7E_b3YwUa5",
        "colab_type": "text"
      },
      "source": [
        "CNN model. I have taken this directly from the PySyft tutorial. We can potentialy look at changig this but I have not spent time on this yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF1AUaDPgeet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnJoOM8NwuSN",
        "colab_type": "text"
      },
      "source": [
        "This is the train function. This is sort of a hack solution to train the model on the 1st dataset using PySyft's federated training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyYlNeVUgkeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, federated_train_loader, optimizer,epoch):\n",
        "    \n",
        "    model.train()\n",
        "        \n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "        cal_grad_bkpropgt(data,target,batch_idx,federated_train_loader,model,device,epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHRQbHYuPT9X",
        "colab_type": "text"
      },
      "source": [
        "Helper function to get delta's from each Agent when training on datasets 2 and 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrFmckeRPMAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_grad_bkpropgt(data,target,batch_idx,federated_train_loader,model,device,epoch):\n",
        "  model.send(data.location) # <-- NEW: send the model to the right location\n",
        "  data, target = data.to(device), target.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  output = model(data)\n",
        "  loss = F.nll_loss(output, target)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  model.get() # <-- NEW: get the model back\n",
        "  if batch_idx % args.log_interval == 0:\n",
        "      loss = loss.get() # <-- NEW: get the loss back\n",
        "      print('Agent: {} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          data.location,epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
        "          100. * batch_idx / len(federated_train_loader), loss.item()))\n",
        "  \n",
        "  #return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zm_MAJywyDO",
        "colab_type": "text"
      },
      "source": [
        "Test fucniton to test model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isHi4SZAk0Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    '''\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    '''\n",
        "    return test_loss, 100. * correct / len(test_loader.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJRkjnFS2een",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "c3f19a3b-2ee0-4a24-8740-c742170d6ffa"
      },
      "source": [
        "output = model(federated_train_ds_t1.batch_samplers[56014])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-a5718238863f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfederated_train_ds_t1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_samplers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m56014\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 56014"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmJMQpgm2VnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test(args,model,device,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n0oDTdixJwL",
        "colab_type": "text"
      },
      "source": [
        "Train model on dataset 1 (remember dataset 1 corresponds to training at timestamp 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4AuXkAIiUL5",
        "colab_type": "code",
        "outputId": "c94438af-f6ed-4b7d-86e3-034efabcedef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1,5):\n",
        "  train(args, model, device, federated_train_ds_t1, optimizer,epoch)\n",
        "  test(args, model, device, test_loader)\n",
        "\n",
        "#if (args.save_model):\n",
        "#    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agent: <VirtualWorker id:Agent_1 #objects:13> Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.318667\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:13> Train Epoch: 1 [1920/40000 (5%)]\tLoss: 2.127270\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:13> Train Epoch: 1 [3840/40000 (10%)]\tLoss: 1.779673\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:13> Train Epoch: 1 [5760/40000 (14%)]\tLoss: 1.175882\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:13> Train Epoch: 1 [7680/40000 (19%)]\tLoss: 0.753973\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 1 [9600/40000 (24%)]\tLoss: 0.613124\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [11520/40000 (29%)]\tLoss: 0.566298\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [13440/40000 (34%)]\tLoss: 0.401585\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [15360/40000 (38%)]\tLoss: 0.580752\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [17280/40000 (43%)]\tLoss: 0.185194\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [19200/40000 (48%)]\tLoss: 0.336762\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 1 [21120/40000 (53%)]\tLoss: 0.463871\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 1 [23040/40000 (58%)]\tLoss: 0.479315\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 1 [24960/40000 (62%)]\tLoss: 0.453104\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 1 [26880/40000 (67%)]\tLoss: 0.249160\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 1 [28800/40000 (72%)]\tLoss: 0.184895\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [30720/40000 (77%)]\tLoss: 0.403146\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [32640/40000 (82%)]\tLoss: 0.134170\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [34560/40000 (86%)]\tLoss: 0.217710\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [36480/40000 (91%)]\tLoss: 0.118043\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [38400/40000 (96%)]\tLoss: 0.222941\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 2 [0/40000 (0%)]\tLoss: 0.276122\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 2 [1920/40000 (5%)]\tLoss: 0.205203\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 2 [3840/40000 (10%)]\tLoss: 0.123459\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 2 [5760/40000 (14%)]\tLoss: 0.225475\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 2 [7680/40000 (19%)]\tLoss: 0.181361\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 2 [9600/40000 (24%)]\tLoss: 0.197578\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 2 [11520/40000 (29%)]\tLoss: 0.249345\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 2 [13440/40000 (34%)]\tLoss: 0.180006\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 2 [15360/40000 (38%)]\tLoss: 0.303429\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 2 [17280/40000 (43%)]\tLoss: 0.619722\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 2 [19200/40000 (48%)]\tLoss: 0.472540\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 2 [21120/40000 (53%)]\tLoss: 0.176575\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 2 [23040/40000 (58%)]\tLoss: 0.140636\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 2 [24960/40000 (62%)]\tLoss: 0.191609\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 2 [26880/40000 (67%)]\tLoss: 0.120129\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 2 [28800/40000 (72%)]\tLoss: 0.269809\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 2 [30720/40000 (77%)]\tLoss: 0.198132\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 2 [32640/40000 (82%)]\tLoss: 0.262665\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 2 [34560/40000 (86%)]\tLoss: 0.320292\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 2 [36480/40000 (91%)]\tLoss: 0.162618\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 2 [38400/40000 (96%)]\tLoss: 0.083027\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 3 [0/40000 (0%)]\tLoss: 0.172179\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 3 [1920/40000 (5%)]\tLoss: 0.236717\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 3 [3840/40000 (10%)]\tLoss: 0.178957\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 3 [5760/40000 (14%)]\tLoss: 0.054794\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 3 [7680/40000 (19%)]\tLoss: 0.094895\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 3 [9600/40000 (24%)]\tLoss: 0.125021\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 3 [11520/40000 (29%)]\tLoss: 0.123902\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 3 [13440/40000 (34%)]\tLoss: 0.080877\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 3 [15360/40000 (38%)]\tLoss: 0.115507\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 3 [17280/40000 (43%)]\tLoss: 0.140622\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 3 [19200/40000 (48%)]\tLoss: 0.106364\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 3 [21120/40000 (53%)]\tLoss: 0.099566\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 3 [23040/40000 (58%)]\tLoss: 0.091980\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 3 [24960/40000 (62%)]\tLoss: 0.151570\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 3 [26880/40000 (67%)]\tLoss: 0.256722\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 3 [28800/40000 (72%)]\tLoss: 0.161443\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 3 [30720/40000 (77%)]\tLoss: 0.108967\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 3 [32640/40000 (82%)]\tLoss: 0.228058\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 3 [34560/40000 (86%)]\tLoss: 0.133230\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 3 [36480/40000 (91%)]\tLoss: 0.192452\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 3 [38400/40000 (96%)]\tLoss: 0.014612\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.153253\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 4 [1920/40000 (5%)]\tLoss: 0.157812\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 4 [3840/40000 (10%)]\tLoss: 0.035692\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 4 [5760/40000 (14%)]\tLoss: 0.076551\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 4 [7680/40000 (19%)]\tLoss: 0.030891\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 4 [9600/40000 (24%)]\tLoss: 0.116716\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 4 [11520/40000 (29%)]\tLoss: 0.158038\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 4 [13440/40000 (34%)]\tLoss: 0.070447\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 4 [15360/40000 (38%)]\tLoss: 0.128359\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 4 [17280/40000 (43%)]\tLoss: 0.175877\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 4 [19200/40000 (48%)]\tLoss: 0.148678\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 4 [21120/40000 (53%)]\tLoss: 0.119495\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 4 [23040/40000 (58%)]\tLoss: 0.167373\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 4 [24960/40000 (62%)]\tLoss: 0.129948\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 4 [26880/40000 (67%)]\tLoss: 0.085782\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 4 [28800/40000 (72%)]\tLoss: 0.163925\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 4 [30720/40000 (77%)]\tLoss: 0.044941\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 4 [32640/40000 (82%)]\tLoss: 0.088208\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 4 [34560/40000 (86%)]\tLoss: 0.227908\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 4 [36480/40000 (91%)]\tLoss: 0.214496\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 4 [38400/40000 (96%)]\tLoss: 0.124802\n",
            "CPU times: user 5min 47s, sys: 19.1 s, total: 6min 6s\n",
            "Wall time: 6min 7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNTMFTQixQUn",
        "colab_type": "text"
      },
      "source": [
        "Function to train on datasets 2 and 3 corresponding to time stamps t2 and t3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6rBjrfmjiaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_subsequent_trainings(args, model,Current_model, device, federated_train_loader, optimizer):\n",
        "\n",
        "  #model.train()\n",
        "     \n",
        "  for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "      \n",
        "      if data.location == Agent_1:\n",
        "        #print('Working on Agent 1')\n",
        "        cal_grad_bkpropgt_return_delta(data,target,batch_idx,federated_train_loader,model,device)\n",
        "          \n",
        "\n",
        "      \n",
        "      elif data.location == Agent_2:\n",
        "        \n",
        "        #print('Working on Agent 2')\n",
        "        delta_Agent_1 = {name: Current_model.state_dict()[name] - model.state_dict()[name] for name in Current_model.state_dict() if name in model.state_dict()}\n",
        "        model.load_state_dict(Current_model.state_dict())\n",
        "        cal_grad_bkpropgt_return_delta(data,target,batch_idx,federated_train_loader,model,device)\n",
        "        \n",
        "      \n",
        "      elif data.location == Agent_3:\n",
        "        #print('Working on Agent 3')\n",
        "        delta_Agent_2 = {name: Current_model.state_dict()[name] - model.state_dict()[name] for name in Current_model.state_dict() if name in model.state_dict()}\n",
        "        model.load_state_dict(Current_model.state_dict())\n",
        "        cal_grad_bkpropgt_return_delta(data,target,batch_idx,federated_train_loader,model,device)\n",
        "        \n",
        "      \n",
        "      elif data.location == Agent_4:\n",
        "        #print('Working on Agent 4')\n",
        "        delta_Agent_3 = {name: Current_model.state_dict()[name] - model.state_dict()[name] for name in Current_model.state_dict() if name in model.state_dict()}\n",
        "        model.load_state_dict(Current_model.state_dict())\n",
        "        cal_grad_bkpropgt_return_delta(data,target,batch_idx,federated_train_loader,model,device)\n",
        "\n",
        "      else:\n",
        "        pass\n",
        "  \n",
        "  delta_Agent_4 = {name: Current_model.state_dict()[name] - model.state_dict()[name] for name in Current_model.state_dict() if name in model.state_dict()}\n",
        "\n",
        "  \n",
        "  return delta_Agent_1, delta_Agent_2, delta_Agent_3, delta_Agent_4    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfE5aesNPcuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_grad_bkpropgt_return_delta(data,target,batch_idx,federated_train_loader,model,device):\n",
        "  #org_model_dict = model.state_dict()\n",
        "  #print('Starting training on batch', batch_idx)\n",
        "  model.send(data.location) # <-- NEW: send the model to the right location\n",
        "  data, target = data.to(device), target.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  output = model(data)\n",
        "  loss = F.nll_loss(output, target)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  model.get() # <-- NEW: get the model back\n",
        "  if batch_idx % args.log_interval == 0:\n",
        "      loss = loss.get() # <-- NEW: get the loss back\n",
        "      print('Agent: {} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          data.location,1, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
        "          100. * batch_idx / len(federated_train_loader), loss.item()))\n",
        "      \n",
        "  #return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXjgUJF2mTvP",
        "colab_type": "code",
        "outputId": "12d18b62-0b65-468f-ad9e-3bcfd8b35066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "Current_model = Net().to(device)\n",
        "Current_model.load_state_dict(model.state_dict())\n",
        "\n",
        "#train_subsequent_trainings(args, model, device, federated_train_ds_t2, optimizer)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xe9rZZojl8x",
        "colab_type": "code",
        "outputId": "b9c84858-0b36-4215-dd93-259f295f3a8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Get deltas from each agent for dataset 2 (corresponding to tiemstamp 2)\n",
        "\n",
        "delta_Agent_1, delta_Agent_2, delta_Agent_3, delta_Agent_4 = train_subsequent_trainings(args, model,Current_model, device, federated_train_ds_t3, optimizer)\n",
        "\n",
        "#test(args, model, device, test_loader)\n",
        "\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.044363\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 1 [1920/8000 (24%)]\tLoss: 0.162713\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [3840/8000 (48%)]\tLoss: 0.061971\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 1 [5760/8000 (72%)]\tLoss: 0.165200\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [7680/8000 (96%)]\tLoss: 0.047698\n",
            "CPU times: user 16.5 s, sys: 822 ms, total: 17.3 s\n",
            "Wall time: 17.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AhD1GD8MRxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Boost malicious agents updates\n",
        "\n",
        "for name,param in delta_Agent_3.items():\n",
        "  delta_Agent_3 = {name: delta_Agent_3[name]*(1.0) for name in delta_Agent_3}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABSlWPsoO9hG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def updated_weights(model,delta):\n",
        "  for name,param in model.state_dict().items():\n",
        "    new_weights = {name: model.state_dict()[name] - delta[name] for name in model.state_dict()}\n",
        "  return new_weights\n",
        "\n",
        "def avg_agent_updates(agent_updates_dict_list):\n",
        "  \n",
        "  all_updates = Counter()\n",
        "  all_param_names = Counter()\n",
        "  for agent_update in agent_updates_dict_list:\n",
        "      all_updates.update(agent_update)\n",
        "      all_param_names.update(agent_update.keys())\n",
        "\n",
        "  averaged_updates_delta = {x: (1.0 * all_updates[x])/all_param_names[x] for x in all_updates.keys()}\n",
        "\n",
        "  return averaged_updates_delta\n",
        "\n",
        "def test_updates(agent_updates,test_loader):\n",
        "  total_number_agents = len(agent_updates)\n",
        "  \n",
        "  for idx in range (0,total_number_agents):\n",
        "    current_agent_delta = agent_updates[idx]\n",
        "    mod_weights = updated_weights(Current_model,current_agent_delta)\n",
        "    Test_model = Net().to(device)\n",
        "    Test_model.load_state_dict(mod_weights)\n",
        "    current_agent_loss, current_agent_accuracy = test(args, Test_model, device, test_loader)\n",
        "    print('Loss if updates included from agent: {}, {:.6f}. Accuracy: {:.2f}'.format(idx+1, current_agent_loss,current_agent_accuracy))\n",
        "    average_delta_other_Agents = avg_agent_updates([x for i,x in enumerate(agent_updates) if i!=idx])\n",
        "    mod_weights = updated_weights(Current_model,average_delta_other_Agents)\n",
        "    Test_model = Net().to(device)\n",
        "    Test_model.load_state_dict(mod_weights)\n",
        "    all_other_agent_loss, all_other_agent_accuracy = test(args, Test_model, device, test_loader)\n",
        "    print('Loss if updates included from all other agents: {:.6f}. Acuracy: {:.2f}'.format(all_other_agent_loss,all_other_agent_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP1tvozTW9el",
        "colab_type": "code",
        "outputId": "1388e3e1-4c77-41b9-b8a3-15ca230ea3fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "%%time\n",
        "test_updates([delta_Agent_1, delta_Agent_2, delta_Agent_3, delta_Agent_4],test_loader)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss if updates included from agent: 0, 0.094762. Accuracy: 97.29\n",
            "Loss if updates included from all other agents: 36.798253. Acuracy: 17.25\n",
            "Loss if updates included from agent: 1, 0.082955. Accuracy: 97.58\n",
            "Loss if updates included from all other agents: 36.817675. Acuracy: 17.48\n",
            "Loss if updates included from agent: 2, 330.884456. Accuracy: 8.92\n",
            "Loss if updates included from all other agents: 0.092422. Acuracy: 97.32\n",
            "Loss if updates included from agent: 3, 0.142409. Accuracy: 95.52\n",
            "Loss if updates included from all other agents: 36.133632. Acuracy: 17.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbzvFLwkoYgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accept_update(update)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}