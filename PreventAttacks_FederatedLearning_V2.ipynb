{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreventAttacks_FederatedLearning_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajattri/CSC591/blob/master/PreventAttacks_FederatedLearning_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHK8rjFlfjl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############\n",
        "#INSTALL PySyft\n",
        "##########\n",
        "\n",
        "!git clone https://github.com/OpenMined/PySyft.git\n",
        "!cd PySyft/\n",
        "!pip install -r PySyft/pip-dep/requirements.txt\n",
        "!pip install -r PySyft/pip-dep/requirements_udacity.txt\n",
        "!python PySyft/setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59BrMbq6ug4K",
        "colab_type": "text"
      },
      "source": [
        "Before Running the next step make sure to \"Restart Runtime\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX-oDlqVftfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell to add PySyft path \n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('./PySyft'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kJ6-PAifwhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKD0tBEGuvWZ",
        "colab_type": "text"
      },
      "source": [
        "Import PySyft and create hook to use torch libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adi3kkgifygA",
        "colab_type": "code",
        "outputId": "7ec59fb1-3e63-429b-ea7d-684e85497a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import syft as sy  # <-- NEW: import the Pysyft library\n",
        "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Torch was already hooked... skipping hooking process\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8BoTzYRu4MA",
        "colab_type": "text"
      },
      "source": [
        "Create Four agents. Each of them will be used to train on the data independently (sequentially though!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nonSaE52gTpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Agent_1 = sy.VirtualWorker(hook, id=\"Agent_1\")\n",
        "Agent_2 = sy.VirtualWorker(hook, id=\"Agent_2\")\n",
        "Agent_3 = sy.VirtualWorker(hook, id=\"Agent_3\")\n",
        "Agent_4 = sy.VirtualWorker(hook, id=\"Agent_4\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KQw4SNvvELE",
        "colab_type": "text"
      },
      "source": [
        "Arguments to be used for training. I have taken this directly from the PySyft tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOHndFeLf04a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 1\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H50kLyJDf-ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download dataset\n",
        "\n",
        "mnist_full_train_dataset = datasets.MNIST('../data', train=True, download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQLIHCsgvQ2s",
        "colab_type": "text"
      },
      "source": [
        "Split the dataset into three parts. This is done to mimic training on the data at three different \"time stamps\". The first dataset will be used to train the model with no malicious data. We will include some malicious data in datasets 2 and 3 and make sure the model obtained from dataset 1 is not updated by a malicious agent's updated delta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3URcXVdggKlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "ds_t1,ds_t2,ds_t3 = torch.utils.data.random_split(mnist_full_train_dataset,(40000,12000,8000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKTmPEeivrdO",
        "colab_type": "text"
      },
      "source": [
        "# TO DO\n",
        "How to manipulate dataset to include malicious updates???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7QEpjlEpkGY",
        "colab_type": "code",
        "outputId": "8abf2307-f6f9-40bf-9bd3-56c33d0e0999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "temp = ds_t2\n",
        "print(ds_t2[10][1])\n",
        "temp[10][1] = 4\n",
        "print(temp[10][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e1d460919987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_t2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_t2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyMVXrL2vy4A",
        "colab_type": "text"
      },
      "source": [
        "Create three \"federated\" datastes. Each corresponding to the dataset created above. This is done to use \"PySyft\" infrastructure to mimic training model using four different slaves (or clients or agents)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR-DEUvogLOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "federated_train_ds_t1 = sy.FederatedDataLoader( ds_t1.federate((Agent_1,Agent_2,Agent_3,Agent_4)),batch_size=64,shuffle=True, **kwargs)\n",
        "federated_train_ds_t2 = sy.FederatedDataLoader( ds_t2.federate((Agent_1,Agent_2,Agent_3,Agent_4)),batch_size=64,shuffle=True, **kwargs)\n",
        "federated_train_ds_t3 = sy.FederatedDataLoader( ds_t3.federate((Agent_1,Agent_2,Agent_3,Agent_4)),batch_size=64,shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4cwnrLhgRld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the dataset that will be used repeateadly to test model performance\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU7E_b3YwUa5",
        "colab_type": "text"
      },
      "source": [
        "CNN model. I have taken this directly from the PySyft tutorial. We can potentialy look at changig this but I have not spent time on this yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF1AUaDPgeet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnJoOM8NwuSN",
        "colab_type": "text"
      },
      "source": [
        "This is the train function. This is sort of a hack solution to train the model on the 1st dataset using PySyft's federated training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyYlNeVUgkeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, federated_train_loader, optimizer,epoch):\n",
        "    \n",
        "    model.train()\n",
        "        \n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "        cal_grad_bkpropgt(data,target,batch_idx,federated_train_loader,model,device,epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zm_MAJywyDO",
        "colab_type": "text"
      },
      "source": [
        "Test fucniton to test model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isHi4SZAk0Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    '''\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    '''\n",
        "    return test_loss, 100. * correct / len(test_loader.dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnhh7jVew1UF",
        "colab_type": "text"
      },
      "source": [
        "Helper function to calculate gradient. This will be used on the 1st dataset when training on non-malicious data to get a basline model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS3IHFvmhbdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_grad_bkpropgt(data,target,batch_idx,federated_train_loader,model,device,epoch):\n",
        "  model.send(data.location) # <-- NEW: send the model to the right location\n",
        "  data, target = data.to(device), target.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  output = model(data)\n",
        "  loss = F.nll_loss(output, target)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  model.get() # <-- NEW: get the model back\n",
        "  if batch_idx % args.log_interval == 0:\n",
        "      loss = loss.get() # <-- NEW: get the loss back\n",
        "      print('Agent: {} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          data.location,epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
        "          100. * batch_idx / len(federated_train_loader), loss.item()))\n",
        "  \n",
        "  #return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acaksfhyxCRJ",
        "colab_type": "text"
      },
      "source": [
        "Helper function to get delta's from each Agent when training on datasets 2 and 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8ix7322hbgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_grad_bkpropgt_return_delta(data,target,batch_idx,federated_train_loader,model,device):\n",
        "  #org_model_dict = model.state_dict()\n",
        "  #print('Starting training on batch', batch_idx)\n",
        "  model.send(data.location) # <-- NEW: send the model to the right location\n",
        "  data, target = data.to(device), target.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  output = model(data)\n",
        "  loss = F.nll_loss(output, target)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  model.get() # <-- NEW: get the model back\n",
        "  if batch_idx % args.log_interval == 0:\n",
        "      loss = loss.get() # <-- NEW: get the loss back\n",
        "      print('Agent: {} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          data.location,1, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
        "          100. * batch_idx / len(federated_train_loader), loss.item()))\n",
        "      \n",
        "  #return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n0oDTdixJwL",
        "colab_type": "text"
      },
      "source": [
        "Train model on dataset 1 (remember dataset 1 corresponds to training at timestamp 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4AuXkAIiUL5",
        "colab_type": "code",
        "outputId": "8ecee6c2-f5ad-4284-94c1-2c5be9ae7e7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
        "\n",
        "for epoch in range(1,5):\n",
        "  train(args, model, device, federated_train_ds_t1, optimizer,epoch)\n",
        "  test(args, model, device, test_loader)\n",
        "\n",
        "#if (args.save_model):\n",
        "#    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agent: <VirtualWorker id:Agent_1 #objects:15> Train Epoch: 1 [0/40000 (0%)]\tLoss: 2.316348\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:15> Train Epoch: 1 [1920/40000 (5%)]\tLoss: 2.128003\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:15> Train Epoch: 1 [3840/40000 (10%)]\tLoss: 1.777332\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:15> Train Epoch: 1 [5760/40000 (14%)]\tLoss: 1.170910\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:15> Train Epoch: 1 [7680/40000 (19%)]\tLoss: 0.746679\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 1 [9600/40000 (24%)]\tLoss: 0.570608\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [11520/40000 (29%)]\tLoss: 0.566394\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [13440/40000 (34%)]\tLoss: 0.395439\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [15360/40000 (38%)]\tLoss: 0.581166\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [17280/40000 (43%)]\tLoss: 0.185829\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [19200/40000 (48%)]\tLoss: 0.330292\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 1 [21120/40000 (53%)]\tLoss: 0.453839\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 1 [23040/40000 (58%)]\tLoss: 0.477778\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 1 [24960/40000 (62%)]\tLoss: 0.369381\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 1 [26880/40000 (67%)]\tLoss: 0.244642\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 1 [28800/40000 (72%)]\tLoss: 0.179368\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [30720/40000 (77%)]\tLoss: 0.316987\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [32640/40000 (82%)]\tLoss: 0.124849\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [34560/40000 (86%)]\tLoss: 0.205997\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [36480/40000 (91%)]\tLoss: 0.106704\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [38400/40000 (96%)]\tLoss: 0.217766\n",
            "\n",
            "Test set: Average loss: 0.4698, Accuracy: 8512/10000 (85%)\n",
            "\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 2 [0/40000 (0%)]\tLoss: 0.292218\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 2 [1920/40000 (5%)]\tLoss: 0.197684\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 2 [3840/40000 (10%)]\tLoss: 0.105961\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 2 [5760/40000 (14%)]\tLoss: 0.216900\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 2 [7680/40000 (19%)]\tLoss: 0.180947\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 2 [9600/40000 (24%)]\tLoss: 0.179615\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 2 [11520/40000 (29%)]\tLoss: 0.249978\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 2 [13440/40000 (34%)]\tLoss: 0.177924\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 2 [15360/40000 (38%)]\tLoss: 0.298029\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 2 [17280/40000 (43%)]\tLoss: 0.294809\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 2 [19200/40000 (48%)]\tLoss: 0.242838\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 2 [21120/40000 (53%)]\tLoss: 0.163638\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 2 [23040/40000 (58%)]\tLoss: 0.124758\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 2 [24960/40000 (62%)]\tLoss: 0.190542\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 2 [26880/40000 (67%)]\tLoss: 0.109878\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 2 [28800/40000 (72%)]\tLoss: 0.285472\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 2 [30720/40000 (77%)]\tLoss: 0.210436\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 2 [32640/40000 (82%)]\tLoss: 0.180391\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 2 [34560/40000 (86%)]\tLoss: 0.227320\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 2 [36480/40000 (91%)]\tLoss: 0.073962\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 2 [38400/40000 (96%)]\tLoss: 0.090505\n",
            "\n",
            "Test set: Average loss: 0.1438, Accuracy: 9563/10000 (96%)\n",
            "\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 3 [0/40000 (0%)]\tLoss: 0.159294\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 3 [1920/40000 (5%)]\tLoss: 0.225784\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 3 [3840/40000 (10%)]\tLoss: 0.157679\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 3 [5760/40000 (14%)]\tLoss: 0.053472\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 3 [7680/40000 (19%)]\tLoss: 0.088724\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 3 [9600/40000 (24%)]\tLoss: 0.022841\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 3 [11520/40000 (29%)]\tLoss: 0.120790\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 3 [13440/40000 (34%)]\tLoss: 0.076586\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 3 [15360/40000 (38%)]\tLoss: 0.117141\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 3 [17280/40000 (43%)]\tLoss: 0.130734\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 3 [19200/40000 (48%)]\tLoss: 0.098384\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 3 [21120/40000 (53%)]\tLoss: 0.110247\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 3 [23040/40000 (58%)]\tLoss: 0.086217\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 3 [24960/40000 (62%)]\tLoss: 0.118420\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 3 [26880/40000 (67%)]\tLoss: 0.227569\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 3 [28800/40000 (72%)]\tLoss: 0.132176\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 3 [30720/40000 (77%)]\tLoss: 0.110230\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 3 [32640/40000 (82%)]\tLoss: 0.137945\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 3 [34560/40000 (86%)]\tLoss: 0.130468\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 3 [36480/40000 (91%)]\tLoss: 0.095215\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 3 [38400/40000 (96%)]\tLoss: 0.011879\n",
            "\n",
            "Test set: Average loss: 0.1006, Accuracy: 9701/10000 (97%)\n",
            "\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 4 [0/40000 (0%)]\tLoss: 0.144529\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 4 [1920/40000 (5%)]\tLoss: 0.156998\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 4 [3840/40000 (10%)]\tLoss: 0.029888\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 4 [5760/40000 (14%)]\tLoss: 0.076313\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 4 [7680/40000 (19%)]\tLoss: 0.028924\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 4 [9600/40000 (24%)]\tLoss: 0.111532\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 4 [11520/40000 (29%)]\tLoss: 0.133762\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 4 [13440/40000 (34%)]\tLoss: 0.062296\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 4 [15360/40000 (38%)]\tLoss: 0.118564\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 4 [17280/40000 (43%)]\tLoss: 0.086825\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 4 [19200/40000 (48%)]\tLoss: 0.064747\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 4 [21120/40000 (53%)]\tLoss: 0.118036\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 4 [23040/40000 (58%)]\tLoss: 0.127792\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 4 [24960/40000 (62%)]\tLoss: 0.131460\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 4 [26880/40000 (67%)]\tLoss: 0.091916\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 4 [28800/40000 (72%)]\tLoss: 0.155360\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 4 [30720/40000 (77%)]\tLoss: 0.026667\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 4 [32640/40000 (82%)]\tLoss: 0.083428\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 4 [34560/40000 (86%)]\tLoss: 0.121766\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 4 [36480/40000 (91%)]\tLoss: 0.226892\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 4 [38400/40000 (96%)]\tLoss: 0.113399\n",
            "\n",
            "Test set: Average loss: 0.0882, Accuracy: 9722/10000 (97%)\n",
            "\n",
            "CPU times: user 5min 51s, sys: 15.9 s, total: 6min 7s\n",
            "Wall time: 6min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNTMFTQixQUn",
        "colab_type": "text"
      },
      "source": [
        "Function to train on datasets 2 and 3 corresponding to time stamps t2 and t3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6rBjrfmjiaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_subsequent_trainings(args, model,Current_model, device, federated_train_loader, optimizer):\n",
        "\n",
        "  #model.train()\n",
        "     \n",
        "  for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
        "      \n",
        "      if data.location == Agent_1:\n",
        "        #print('Working on Agent 1')\n",
        "        cal_grad_bkpropgt_return_delta(data,target,batch_idx,federated_train_loader,model,device)\n",
        "          \n",
        "\n",
        "      \n",
        "      elif data.location == Agent_2:\n",
        "        \n",
        "        #print('Working on Agent 2')\n",
        "        delta_Agent_1 = {name: Current_model.state_dict()[name] - model.state_dict()[name] for name in Current_model.state_dict() if name in model.state_dict()}\n",
        "        model.load_state_dict(Current_model.state_dict())\n",
        "        cal_grad_bkpropgt_return_delta(data,target,batch_idx,federated_train_loader,model,device)\n",
        "        \n",
        "      \n",
        "      elif data.location == Agent_3:\n",
        "        #print('Working on Agent 3')\n",
        "        delta_Agent_2 = {name: Current_model.state_dict()[name] - model.state_dict()[name] for name in Current_model.state_dict() if name in model.state_dict()}\n",
        "        model.load_state_dict(Current_model.state_dict())\n",
        "        cal_grad_bkpropgt_return_delta(data,target,batch_idx,federated_train_loader,model,device)\n",
        "        \n",
        "      \n",
        "      elif data.location == Agent_4:\n",
        "        #print('Working on Agent 4')\n",
        "        delta_Agent_3 = {name: Current_model.state_dict()[name] - model.state_dict()[name] for name in Current_model.state_dict() if name in model.state_dict()}\n",
        "        model.load_state_dict(Current_model.state_dict())\n",
        "        cal_grad_bkpropgt_return_delta(data,target,batch_idx,federated_train_loader,model,device)\n",
        "\n",
        "      else:\n",
        "        pass\n",
        "  \n",
        "  delta_Agent_4 = {name: Current_model.state_dict()[name] - model.state_dict()[name] for name in Current_model.state_dict() if name in model.state_dict()}\n",
        "\n",
        "  \n",
        "  return delta_Agent_1, delta_Agent_2, delta_Agent_3, delta_Agent_4    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXjgUJF2mTvP",
        "colab_type": "code",
        "outputId": "b8eb558c-455f-493f-ab68-478981ad3d5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "Current_model = Net().to(device)\n",
        "Current_model.load_state_dict(model.state_dict())\n",
        "\n",
        "#train_subsequent_trainings(args, model, device, federated_train_ds_t2, optimizer)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xe9rZZojl8x",
        "colab_type": "code",
        "outputId": "7e670a04-6320-4281-ffb4-2d2d9b1d8388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Get deltas from each agent for dataset 2 (corresponding to tiemstamp 2)\n",
        "\n",
        "delta_Agent_1, delta_Agent_2, delta_Agent_3, delta_Agent_4 = train_subsequent_trainings(args, model,Current_model, device, federated_train_ds_t2, optimizer)\n",
        "\n",
        "#test(args, model, device, test_loader)\n",
        "\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 1 [0/12032 (0%)]\tLoss: 0.058366\n",
            "Agent: <VirtualWorker id:Agent_1 #objects:11> Train Epoch: 1 [1920/12032 (16%)]\tLoss: 0.063051\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [3840/12032 (32%)]\tLoss: 0.090062\n",
            "Agent: <VirtualWorker id:Agent_2 #objects:11> Train Epoch: 1 [5760/12032 (48%)]\tLoss: 0.020690\n",
            "Agent: <VirtualWorker id:Agent_3 #objects:11> Train Epoch: 1 [7680/12032 (64%)]\tLoss: 0.111549\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [9600/12032 (80%)]\tLoss: 0.047126\n",
            "Agent: <VirtualWorker id:Agent_4 #objects:11> Train Epoch: 1 [11520/12032 (96%)]\tLoss: 0.138646\n",
            "CPU times: user 24.3 s, sys: 1.14 s, total: 25.5 s\n",
            "Wall time: 25.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABSlWPsoO9hG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def updated_weights(model,delta):\n",
        "  for name,param in model.state_dict().items():\n",
        "    new_weights = {name: model.state_dict()[name] - delta[name] for name in model.state_dict()}\n",
        "  return new_weights\n",
        "\n",
        "def avg_agent_updates(agent_updates_dict_list):\n",
        "  \n",
        "  all_updates = Counter()\n",
        "  all_param_names = Counter()\n",
        "  for agent_update in agent_updates_dict_list:\n",
        "      all_updates.update(agent_update)\n",
        "      all_param_names.update(agent_update.keys())\n",
        "\n",
        "  averaged_updates_delta = {x: (1.0 * all_updates[x])/all_param_names[x] for x in all_updates.keys()}\n",
        "\n",
        "  return averaged_updates_delta\n",
        "\n",
        "def test_updates(agent_updates,test_loader):\n",
        "  total_number_agents = len(agent_updates)\n",
        "  \n",
        "  for idx in range (0,total_number_agents):\n",
        "    current_agent_delta = agent_updates[idx]\n",
        "    mod_weights = updated_weights(Current_model,current_agent_delta)\n",
        "    Test_model.load_state_dict(mod_weights)\n",
        "    current_agent_loss, current_agent_accuracy = test(args, Test_model, device, test_loader)\n",
        "    print('Loss if updates included from agent: {}, {:.6f}. Accuracy: {:.2f}'.format(idx, current_agent_loss,current_agent_accuracy))\n",
        "    average_delta_other_Agents = avg_agent_updates([x for i,x in enumerate(agent_updates) if i!=idx])\n",
        "    mod_weights = updated_weights(Current_model,average_delta_other_Agents)\n",
        "    Test_model.load_state_dict(mod_weights)\n",
        "    all_other_agent_loss, all_other_agent_accuracy = test(args, Test_model, device, test_loader)\n",
        "    print('Loss if updates included from all other agents: {:.6f}. Acuracy: {:.2f}'.format(all_other_agent_loss,all_other_agent_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP1tvozTW9el",
        "colab_type": "code",
        "outputId": "26fc012d-5b99-44f1-c659-8e8033d7d3e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "test_updates([delta_Agent_1, delta_Agent_2, delta_Agent_3, delta_Agent_4],test_loader)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss if updates included from agent: 0 0.084581. Accuracy: 97.48\n",
            "Loss if updates included from all other agents: 0.07483155288696289. Acuracy: 97.88\n",
            "Loss if updates included from agent: 1 0.078709. Accuracy: 97.55\n",
            "Loss if updates included from all other agents: 0.07650294570922851. Acuracy: 97.76\n",
            "Loss if updates included from agent: 2 0.077368. Accuracy: 97.81\n",
            "Loss if updates included from all other agents: 0.07810172119140625. Acuracy: 97.70\n",
            "Loss if updates included from agent: 3 0.080095. Accuracy: 97.55\n",
            "Loss if updates included from all other agents: 0.07612780494689941. Acuracy: 97.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbzvFLwkoYgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}